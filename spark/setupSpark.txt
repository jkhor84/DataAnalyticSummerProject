Set up Spark on Linux
1) Install Java
    • Update system packages
        ◦ sudo apt update && sudo apt -y full-upgrade
        ◦ [ -f /var/run/reboot-required ] && sudo reboot -f
    • Install Java Runtime
        ◦ sudo apt install curl mlocate default-jdk -y
        ◦ java -version
    • Example of java -version
        ◦ java -version
          openjdk version "11.0.15" 2022-04-19
          OpenJDK Runtime Environment (build 11.0.15+10-Ubuntu-0ubuntu0.22.04.1)
          OpenJDK 64-Bit Server VM (build 11.0.15+10-Ubuntu-0ubuntu0.22.04.1, mixed mode, sharing)

2) Download Apache Spark
    • Get a URL for the latest Apache Spark from (https://spark.apache.org/downloads.html).
        ◦ Example: https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz
    • Download Apache Spark from web to Linux
        ◦ wget https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz
    • Unzip the downloaded tgz file
        ◦ tar xvf spark-3.3.0-bin-hadoop3.tgz
    • Move the Spark folder created after extraction to the /opt/ directory.
        ◦ sudo mv spark-3.3.0-bin-hadoop3/ /opt/spark

3) Set Spark environment
    • Setup environment variables in ~bashrc file
        ◦ nano ~/.bashrc
        ◦ Include the following environment variables in this file:
            ▪ export SPARK_HOME=/opt/spark
              export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
        ◦ source ~/.bashrc

4) Start a master server
    • start-master.sh 
        ◦ It will look similar to below once the code above, start-master.sh, is executed
            ▪ starting org.apache.spark.deploy.master.Master, logging to /home/jasper/spark/logs/spark-jasper-org.apache.spark.deploy.master.Master-1-jasper-Latitude-5400.out
    • sudo ss -tunelp | grep 8080
        ◦ It will look similar to below once the code above, sudo ss -tunelp | grep 8080, is executed
            ▪ tcp   LISTEN 0      1                                     *:8080             *:*    users:(("java",pid=251876,fd=319)) uid:1000 ino:1649267 sk:18 cgroup:/user.slice/user-1000.slice/session-29.scope v6only:0 <->   

5) Start a Spark Worker Process
    • start-slave.sh
        ◦ It will look similar to below once the code above, start-slave.sh, is executed
            ▪ starting org.apache.spark.deploy.worker.Worker, logging to /home/jasper/spark/logs/spark-jasper-org.apache.spark.deploy.worker.Worker-1-jasper-Latitude-5400.out

6) Use a Spark shell
    • /opt/spark/bin/spark-shell
        ◦ It will look similar to below once the code above, /opt/spark/bin/spark-shell, is executed
            ▪ 22/06/28 21:11:18 WARN Utils: Your hostname, jasper-Latitude-5400 resolves to a loopback address: 127.0.1.1; using 192.168.12.176 instead (on interface wlo1)
              22/06/28 21:11:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
              Setting default log level to "WARN".
              To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
              22/06/28 21:11:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
              Spark context Web UI available at http://jasper-Latitude-5400.lan:4040
              Spark context available as 'sc' (master = local[*], app id = local-1656468684629).
              Spark session available as 'spark'.
              Welcome to
                    ____              __
                   / __/__  ___ _____/ /__
                  _\ \/ _ \/ _ `/ __/  '_/
                 /___/ .__/\_,_/_/ /_/\_\   version 3.3.0
                    /_/
                       
              Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 1.8.0_312)
              
